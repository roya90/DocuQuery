{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4a5xCVLHShapcJPHhLMAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0500ba0a1704d3a910aeb57b39dcd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a0f07f9b7894678bd1928857fb84cb7",
              "IPY_MODEL_3f383b3bfe544f37bdfe64c3b8578609",
              "IPY_MODEL_a692e67153dc42d9b4f296b729bef25a"
            ],
            "layout": "IPY_MODEL_7283d282f1dd4abc9bc4d4f84eb6f7af"
          }
        },
        "1a0f07f9b7894678bd1928857fb84cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_207439edc9f1416bbc74ef56155e950e",
            "placeholder": "​",
            "style": "IPY_MODEL_4f3766ad8ded42d7817dee1752e9711c",
            "value": "Batches: 100%"
          }
        },
        "3f383b3bfe544f37bdfe64c3b8578609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cdf1c01b6e94fef942faea574d1b469",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_043b8fe223614ac2a967c6ebba246c78",
            "value": 1
          }
        },
        "a692e67153dc42d9b4f296b729bef25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09ee3f2939e749a187bde3287f310c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_f2fa0c3bf72c442ab0fe8776d9275497",
            "value": " 1/1 [00:01&lt;00:00,  1.13s/it]"
          }
        },
        "7283d282f1dd4abc9bc4d4f84eb6f7af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "207439edc9f1416bbc74ef56155e950e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f3766ad8ded42d7817dee1752e9711c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cdf1c01b6e94fef942faea574d1b469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "043b8fe223614ac2a967c6ebba246c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09ee3f2939e749a187bde3287f310c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fa0c3bf72c442ab0fe8776d9275497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roya90/DocuQuery/blob/main/Building_a_PDF_Question_Answering_System_with_Retrieval_Augmented_Generation_(RAG)_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3F_hDQIWCMX",
        "outputId": "5f0c922b-ca30-4732-abae-db51952a80a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q PyMuPDF  # PyMuPDF (FAISS dependencies are usually pre-installed in Colab)\n",
        "!pip install -q sentence-transformers faiss-cpu google-generativeai tqdm python-dotenv\n",
        "!pip install -q --upgrade google-cloud-aiplatform\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import fitz  # PyMuPDF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm.notebook import tqdm  # Use notebook version for Colab\n",
        "import spacy\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import files, output  # For file uploads and output control\n",
        "import vertexai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"Libraries installed and imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Spacy Model"
      ],
      "metadata": {
        "id": "dVQ0FFuxWUM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the spacy model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"spaCy model downloaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjsQEzVSWRd9",
        "outputId": "2826c3d1-a5b5-4797-a53d-8fe569c8edeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "spaCy model downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate to Google Cloud Generative AI"
      ],
      "metadata": {
        "id": "iZBWw8NOrxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-genai\n",
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hswCfKMLrwwc",
        "outputId": "52466dda-3a6f-4e53-96b0-634e8beb278f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.27.0)\n",
            "Requirement already satisfied: httpx<1.0.0dev,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.10.6)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.0dev,>=13.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (14.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0dev,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.12.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0dev,>=0.28.1->google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0dev,>=0.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0dev,>=0.28.1->google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0dev,>=0.28.1->google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0dev,>=0.28.1->google-genai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0dev,>=0.28.1->google-genai) (1.3.1)\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=WJDvtqThI3qJvbp7HQlpJE6OwdTzFb&prompt=consent&token_usage=remote&access_type=offline&code_challenge=l4vE_ACMI3hUn1-MtVKF2Jtlf8_CBc9yArs3CsaGzqI&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AQSTgQExG7Sl_vDZq2b5iWg8QY338TxSyDBIXtQ42FTG-05Yspd6ZcGSu327j-41gQmSdw\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Utility Functions (Text Extraction, Chunking, Embedding)"
      ],
      "metadata": {
        "id": "YhFNz8TJn2YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        pdf_path = Path(pdf_path)\n",
        "        if not pdf_path.is_file() or not pdf_path.suffix.lower() == '.pdf':\n",
        "            raise ValueError(\"The provided file is not a valid PDF.\")\n",
        "\n",
        "        text = \"\"\n",
        "        with fitz.open(pdf_path) as pdf_document:\n",
        "            for page_num in range(len(pdf_document)):\n",
        "                text += pdf_document[page_num].get_text()\n",
        "        return text\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"The specified PDF file was not found.\")\n",
        "        return None\n",
        "    except fitz.FileDataError:\n",
        "        print(\"The PDF file is corrupted or unreadable.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Text Chunking ---\n",
        "\n",
        "try:\n",
        "    SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "def is_meaningful(sentence, threshold=5):\n",
        "    sentence = sentence.strip()\n",
        "    if len(sentence) < threshold:\n",
        "        return False\n",
        "    if re.fullmatch(r\"[\\W\\d_]+\", sentence):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def validate_text_input(text, max_length=1_000_000):\n",
        "    if not isinstance(text, str):\n",
        "        raise ValueError(\"Input text must be a string.\")\n",
        "    if len(text) > max_length:\n",
        "        raise ValueError(\"Input text is too large to process.\")\n",
        "    return text.strip()\n",
        "\n",
        "def smart_chunk_spacy_by_paragraph(text):\n",
        "    text = validate_text_input(text)\n",
        "    paragraphs = [para.strip() for para in text.split(\"\\n\") if is_meaningful(para)]\n",
        "    return paragraphs\n",
        "\n",
        "def smart_chunk_spacy(text):\n",
        "    text = validate_text_input(text)\n",
        "    doc = SPACY_NLP(text)\n",
        "    sentences = [sent.text for sent in doc.sents if is_meaningful(sent.text)]\n",
        "    return sentences\n",
        "\n",
        "def smart_chunk_spacy_advanced(text, min_chunk_length=50, max_chunk_length=500):\n",
        "    text = validate_text_input(text)\n",
        "    raw_paragraphs = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).split(\"\\n\\n\")\n",
        "    refined_paragraphs = []\n",
        "    for paragraph in raw_paragraphs:\n",
        "        if len(paragraph.strip()) < min_chunk_length:\n",
        "            continue\n",
        "        doc = SPACY_NLP(paragraph)\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        for sent in doc.sents:\n",
        "            sent_text = sent.text.strip()\n",
        "            if current_length + len(sent_text) > max_chunk_length:\n",
        "                refined_paragraphs.append(\" \".join(current_chunk).strip())\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "            current_chunk.append(sent_text)\n",
        "            current_length += len(sent_text)\n",
        "        if current_chunk:\n",
        "            refined_paragraphs.append(\" \".join(current_chunk).strip())\n",
        "    return refined_paragraphs\n",
        "\n",
        "def smart_chunk_transformers(text, max_tokens=128):\n",
        "    text = validate_text_input(text)\n",
        "    tokens = TOKENIZER(text, truncation=False, return_tensors=\"pt\")\n",
        "    chunks = [text[i:i+max_tokens] for i in range(0, len(tokens['input_ids'][0]), max_tokens)]\n",
        "    return chunks\n",
        "\n",
        "# --- Vector Database (FAISS) Utilities ---\n",
        "from pathlib import Path\n",
        "# Load the embedding model globally\n",
        "try:\n",
        "    MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load the embedding model: {e}\")\n",
        "\n",
        "def validate_text_chunks(text):\n",
        "    if isinstance(text, str):\n",
        "        text = [text]\n",
        "    if not isinstance(text, list) or not all(isinstance(t, str) for t in text):\n",
        "        raise ValueError(\"Input must be a string or a list of strings.\")\n",
        "    return [t.strip() for t in text if t.strip()]\n",
        "\n",
        "def generate_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    chunks = validate_text_chunks(chunks)\n",
        "    embeddings = MODEL.encode(chunks, convert_to_tensor=False, show_progress_bar=True) # Add progress bar\n",
        "    embeddings = np.array(embeddings)\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    normalized_embeddings = embeddings / (norms + 1e-10)  # Add small value for numerical stability\n",
        "    return normalized_embeddings\n",
        "\n",
        "def store_in_faiss(embeddings, db_file=\"vector_db_cosine.index\"):\n",
        "    try:\n",
        "        dimension = embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)  # Use inner product (for cosine similarity with normalized vectors)\n",
        "        index.add(embeddings)\n",
        "        faiss.write_index(index, str(db_file)) #cast to string for cross-platform\n",
        "        print(f\"FAISS index saved to {db_file}\")\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to store FAISS index: {e}\")\n",
        "\n",
        "def load_faiss_index(db_file):\n",
        "    try:\n",
        "        db_file = Path(db_file).resolve()\n",
        "        return faiss.read_index(str(db_file))\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load FAISS index: {e}\")\n",
        "\n",
        "def query_faiss_index(query_text, vector_index, chunks, model_name=\"all-MiniLM-L6-v2\", top_k=2):\n",
        "    try:\n",
        "        query_text = validate_text_chunks(query_text)\n",
        "        if not query_text:\n",
        "            raise ValueError(\"Query text cannot be empty.\")\n",
        "        query_embedding = MODEL.encode(query_text, convert_to_tensor=False)\n",
        "        query_embedding = np.array(query_embedding)\n",
        "        query_embedding = query_embedding / (np.linalg.norm(query_embedding, axis=1, keepdims=True) + 1e-10)\n",
        "        distances, indices = vector_index.search(query_embedding, top_k)\n",
        "        results = [(chunks[idx], distances[0][i], idx) for i, idx in enumerate(indices[0])]\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to query FAISS index: {e}\")\n",
        "\n",
        "print(\"Utility functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIFj92C-nwfm",
        "outputId": "97968744-ac05-4622-8788-70a794ed9db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utility functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Gemini Query Function"
      ],
      "metadata": {
        "id": "FueuQkhin8R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "def query_flash(question = \"What is APR?\", context_chunks = [], model_name=\"gemini-pro\", top_k=3):\n",
        "    \"\"\"\n",
        "    Queries a Large Language Model (LLM) with a question and relevant context chunks.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to be answered.\n",
        "        context_chunks (list): A list of tuples, each containing (text_chunk, similarity_score, index).\n",
        "        model_name (str): The name of the Gemini model to use. Defaults to \"gemini-pro\".\n",
        "        top_k (int): The number of top context chunks to use. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated answer and the relevant context chunks.\n",
        "              Returns an error message as a string in case of exceptions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not isinstance(question, str) or not question.strip():\n",
        "            raise ValueError(\"The question must be a non-empty string.\")\n",
        "        if not isinstance(context_chunks, list) or not all(\n",
        "            isinstance(chunk, (list, tuple)) for chunk in context_chunks\n",
        "        ):\n",
        "            raise ValueError(\"Context chunks must be a list of tuples or lists.\")\n",
        "\n",
        "        client = genai.Client(\n",
        "            vertexai=True,\n",
        "            project=userdata.get('project_id'),\n",
        "            location=location,\n",
        "        )\n",
        "\n",
        "        # Construct the context\n",
        "        context = \" \".join([context_chunk[0] for context_chunk in context_chunks])\n",
        "\n",
        "        # Prompt Engineering\n",
        "        prompt = (\n",
        "            f\"You are a legal assistant specializing in contracts. \"\n",
        "            f\"Answer the question based on the following context, and cite the sources explicitly. \"\n",
        "            f\"Do not include any information not present in the provided context.\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\n\"\n",
        "            f\"Question: {question}\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "        model = \"gemini-2.0-flash-001\"\n",
        "        contents = [prompt\n",
        "        ]\n",
        "        generate_content_config = types.GenerateContentConfig(\n",
        "            temperature = 1,\n",
        "            top_p = 0.95,\n",
        "            max_output_tokens = 8192,\n",
        "            response_modalities = [\"TEXT\"],\n",
        "            safety_settings = [types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "            threshold=\"OFF\"\n",
        "            ),types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "            threshold=\"OFF\"\n",
        "            ),types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "            threshold=\"OFF\"\n",
        "            ),types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "            threshold=\"OFF\"\n",
        "            )],\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # Generate content (streaming)\n",
        "        response_stream = client.models.generate_content_stream(model = model,\n",
        "                    contents = contents,\n",
        "                    config = generate_content_config,)\n",
        "\n",
        "        # Collect the streamed response\n",
        "        generated_answer = \"\"\n",
        "        for chunk in response_stream:\n",
        "            generated_answer += chunk.text\n",
        "\n",
        "        relevant_chunks = context_chunks[:top_k]  # Top k chunks for citation\n",
        "        return {\"answer\": generated_answer.strip(), \"relevant_context\": relevant_chunks}\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "\n",
        "print(\"Gemini query function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60yNWmjpn4dx",
        "outputId": "2b725c0d-99f6-4c3b-d0e5-d70da499864f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini query function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Question Answering Function"
      ],
      "metadata": {
        "id": "4Dqo7GOhoBcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(pdf_path, query_text, relevance_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Main function to perform question answering on a PDF document.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.  (Will be a temporary path in Colab)\n",
        "        query_text (str): The question to ask.\n",
        "        relevance_threshold (float): Minimum similarity score for a chunk.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nExtracting text from the PDF...\")\n",
        "    try:\n",
        "        extracted_text = extract_text_from_pdf(pdf_path)\n",
        "        print(\"Text extracted successfully.\")\n",
        "        if not extracted_text:\n",
        "            print(\"Error: Failed to extract text. Document might be empty/unreadable.\")\n",
        "            return  # Exit if extraction fails\n",
        "\n",
        "        print(\"\\nChunking the extracted text...\")\n",
        "        chunks = smart_chunk_spacy_advanced(extracted_text)  # Use advanced chunker\n",
        "        if not chunks:\n",
        "            print(\"Error: Failed to create text chunks.\")\n",
        "            return\n",
        "        print(f\"Text chunked into {len(chunks)} chunks.\")\n",
        "\n",
        "        print(\"\\nGenerating embeddings for the chunks...\")\n",
        "        embeddings = generate_embeddings(chunks)\n",
        "        print(\"Embeddings generated.\")\n",
        "\n",
        "        print(\"\\nStoring embeddings in FAISS index...\")\n",
        "        index = store_in_faiss(embeddings)  # Use default filename\n",
        "        print(\"FAISS index created and stored.\")\n",
        "\n",
        "        # No need to reload the index immediately, we just created it!\n",
        "\n",
        "        print(f\"\\nQuerying FAISS index with: '{query_text}'...\")\n",
        "        results = query_faiss_index(query_text, index, chunks, top_k=5)\n",
        "\n",
        "        # Filter results by relevance threshold\n",
        "        context_chunks = [(text, dis, idx) for text, dis, idx in results if dis > relevance_threshold]\n",
        "\n",
        "        print(\"\\nResults from FAISS index:\")\n",
        "        for idx, (text, score, doc_id) in enumerate(results):\n",
        "          if score > relevance_threshold:\n",
        "            print(f\"{idx}. Score: {score:.4f}, Document ID: {doc_id}\\n{text[:200]}...\\n\")\n",
        "        print(f\"Found {len(context_chunks)} relevant context chunks.\")\n",
        "\n",
        "\n",
        "        print(\"\\nQuerying the Gemini model for an answer...\")\n",
        "        answer = query_flash(query_text, context_chunks)  # Use Gemini query\n",
        "        if isinstance(answer, str) and \"Error\" in answer: #check for errors from query_flash\n",
        "          print(f\"Error querying Gemini: {answer}\")\n",
        "          return\n",
        "\n",
        "        print(\"\\nGenerated Answer:\")\n",
        "        print(answer[\"answer\"])\n",
        "        print(\"\\nCited Context:\")\n",
        "        for text, _, idx in answer[\"relevant_context\"]:\n",
        "            print(f\"\\tSource {idx}: {text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred in main(): {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Main function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi4_cthJn_RP",
        "outputId": "64cfee0a-4f4f-4b37-f3ae-97de7bd3324e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get API Key, Project ID, and Location, Upload PDF, and Run!"
      ],
      "metadata": {
        "id": "846edhDIoGTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get project ID\n",
        "PROJECT_ID = ! gcloud config get project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\"\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "    print(f\"Please set the project ID manually below\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Pb2JDyLnFA",
        "outputId": "664dac01-5e58-474e-d5e8-4677ca7aea0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please set the project ID manually below\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define project information\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# generate an unique id for this session\n",
        "from datetime import datetime\n",
        "\n",
        "UID = datetime.now().strftime(\"%m%d%H%M\")"
      ],
      "metadata": {
        "id": "S_PTzUklLrV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI and genai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# File upload\n",
        "print(\"Upload your PDF file:\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    print(\"No file uploaded.  Exiting.\")\n",
        "    sys.exit(1)  # Exit if no file\n",
        "\n",
        "pdf_filename = list(uploaded.keys())[0]  # Get the filename\n",
        "print(f\"Uploaded file: {pdf_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "s7Y8WCaFoG_B",
        "outputId": "bceab104-3908-44ba-9af5-33e6700911a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-509ef96f-b046-4ae2-bb18-6dc5381d060b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-509ef96f-b046-4ae2-bb18-6dc5381d060b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving example_doc.pdf to example_doc (5).pdf\n",
            "Uploaded file: example_doc (5).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the question\n",
        "query = input(\"Enter your question: \")  #@param {type:\"string\"}\n",
        "\n",
        "# Run the main function\n",
        "main(pdf_filename, query)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0500ba0a1704d3a910aeb57b39dcd72",
            "1a0f07f9b7894678bd1928857fb84cb7",
            "3f383b3bfe544f37bdfe64c3b8578609",
            "a692e67153dc42d9b4f296b729bef25a",
            "7283d282f1dd4abc9bc4d4f84eb6f7af",
            "207439edc9f1416bbc74ef56155e950e",
            "4f3766ad8ded42d7817dee1752e9711c",
            "4cdf1c01b6e94fef942faea574d1b469",
            "043b8fe223614ac2a967c6ebba246c78",
            "09ee3f2939e749a187bde3287f310c7e",
            "f2fa0c3bf72c442ab0fe8776d9275497"
          ]
        },
        "id": "a9VcNvhTyP_F",
        "outputId": "62cede2a-d7f9-4356-ad8e-cca8c92d4ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: how many days to return an item?\n",
            "\n",
            "Extracting text from the PDF...\n",
            "Text extracted successfully.\n",
            "\n",
            "Chunking the extracted text...\n",
            "Text chunked into 15 chunks.\n",
            "\n",
            "Generating embeddings for the chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0500ba0a1704d3a910aeb57b39dcd72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated.\n",
            "\n",
            "Storing embeddings in FAISS index...\n",
            "FAISS index saved to vector_db_cosine.index\n",
            "FAISS index created and stored.\n",
            "\n",
            "Querying FAISS index with: 'how many days to return an item?'...\n",
            "\n",
            "Results from FAISS index:\n",
            "0. Score: 0.6795, Document ID: 2\n",
            "WAYS TO MAKE A RETURN\n",
            "We accept returns via mail or in store. Returns are eligible for a refund if they are made within 30 days of delivery. Returned items must be presented in the\n",
            "same condition as w...\n",
            "\n",
            "1. Score: 0.6222, Document ID: 11\n",
            "Q: HOW DO I EXCHANGE AN ITEM?\n",
            "A: To exchange an item, you can visit your local Saks Fifth Avenue store and ask a Style Advisor to help you. Q: HOW LONG DO I HAVE TO MAKE A RETURN? A: Returns are eligi...\n",
            "\n",
            "2. Score: 0.5211, Document ID: 7\n",
            "We offer price adjustments within 7 days of the purchase date for full-price items. If you see an item on sale after paying the original price, you can\n",
            "submit a price adjustment request online. Non-U....\n",
            "\n",
            "3. Score: 0.5143, Document ID: 5\n",
            "OUR RETURN POLICY\n",
            "Privacy - Terms\n",
            "START YOUR RETURN\n",
            "2/26/25, 3:40 PM\n",
            "Return Policy | Saks Fifth Avenue\n",
            "https://www.saksfifthavenue.com/c/content/returns-exchanges\n",
            "1/3\n",
            "A $9.95 return shipping fee will ...\n",
            "\n",
            "4. Score: 0.4898, Document ID: 0\n",
            "Earn Rewards With a Saks Credit Card | Get Pre‑Qualified\n",
            "Search\n",
            "SAKS RETURNS\n",
            "You can make a return or gift return by mail or in store for a full refund within 30 days. See our return policy for detail...\n",
            "\n",
            "Found 5 relevant context chunks.\n",
            "\n",
            "Querying the Gemini model for an answer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Answer:\n",
            "Returns are eligible for a full refund if they are made within 30 days of delivery (Return Policy | Saks Fifth Avenue).\n",
            "\n",
            "Cited Context:\n",
            "\tSource 2: WAYS TO MAKE A RETURN\n",
            "We accept returns via mail or in store. Returns are eligible for a refund if they are made within 30 days of delivery. Returned items must be presented in the\n",
            "same condition as when they were received: unworn, undamaged, unaltered, and with the original tags, packaging (if applicable) and proof of purchase. Returns that do not meet these criteria will not be accepted and will be sent back to you with an explanation. Shipping fees cannot be refunded.\n",
            "\tSource 11: Q: HOW DO I EXCHANGE AN ITEM?\n",
            "A: To exchange an item, you can visit your local Saks Fifth Avenue store and ask a Style Advisor to help you. Q: HOW LONG DO I HAVE TO MAKE A RETURN? A: Returns are eligible for a refund if they are made within 30 days of delivery. We cannot guarantee that your return will be accepted if shipped after your\n",
            "return window. Q: HOW DO I MAKE AN INTERNATIONAL RETURN FROM OUTSIDE THE U.S.?\n",
            "\tSource 7: We offer price adjustments within 7 days of the purchase date for full-price items. If you see an item on sale after paying the original price, you can\n",
            "submit a price adjustment request online. Non-U.S. customers can easily make an international return, too. To make sure all of our customers have the best shopping experience, we may restrict or refuse future transactions if we identify an unreasonable return\n",
            "pattern. Q: WHAT IF MY ORDER ARRIVES DAMAGED, BROKEN OR DEFECTIVE? A:\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}