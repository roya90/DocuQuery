{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLLHC5ZGFXGdnQX9txEDfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a0a0363069334ac98aa1c577358e5352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_888b1b06039944eaa5b6960033feca9e",
              "IPY_MODEL_93578cfbe6b144d0bcdedce648242313",
              "IPY_MODEL_a290d7ae94b446318d4f7170a1fe2206"
            ],
            "layout": "IPY_MODEL_2fc78fc04c284f8fa944158b4da9b6ed"
          }
        },
        "888b1b06039944eaa5b6960033feca9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87055f9bffc54e03b82a7456dc430c44",
            "placeholder": "​",
            "style": "IPY_MODEL_3865e6a4d341433a9347e3ffdc8702b2",
            "value": "Batches: 100%"
          }
        },
        "93578cfbe6b144d0bcdedce648242313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_790364264b81459facef7fcb28a967ac",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8561c2e80a14ca3aa6746d092a16911",
            "value": 1
          }
        },
        "a290d7ae94b446318d4f7170a1fe2206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_745e564d11014fe8b6df76a5489bb384",
            "placeholder": "​",
            "style": "IPY_MODEL_a589be05ea53482bb5df103fb115e50f",
            "value": " 1/1 [00:04&lt;00:00,  4.28s/it]"
          }
        },
        "2fc78fc04c284f8fa944158b4da9b6ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87055f9bffc54e03b82a7456dc430c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3865e6a4d341433a9347e3ffdc8702b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "790364264b81459facef7fcb28a967ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8561c2e80a14ca3aa6746d092a16911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "745e564d11014fe8b6df76a5489bb384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a589be05ea53482bb5df103fb115e50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roya90/DocuQuery/blob/main/Building_a_PDF_Question_Answering_System_with_Retrieval_Augmented_Generation_(RAG)_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3F_hDQIWCMX",
        "outputId": "c8e29330-c22d-4b33-f04d-38d7c7619c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q PyMuPDF  # PyMuPDF (FAISS dependencies are usually pre-installed in Colab)\n",
        "!pip install -q sentence-transformers faiss-cpu google-generativeai tqdm python-dotenv\n",
        "!pip install -q --upgrade google-cloud-aiplatform\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import fitz  # PyMuPDF\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm.notebook import tqdm  # Use notebook version for Colab\n",
        "import spacy\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import files, output  # For file uploads and output control\n",
        "import vertexai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "print(\"Libraries installed and imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Spacy Model"
      ],
      "metadata": {
        "id": "dVQ0FFuxWUM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the spacy model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"spaCy model downloaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjsQEzVSWRd9",
        "outputId": "f0e63692-bb4a-49a5-f923-99cb346ce48c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "spaCy model downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate to Google Cloud Generative AI"
      ],
      "metadata": {
        "id": "iZBWw8NOrxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-genai\n",
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hswCfKMLrwwc",
        "outputId": "fd3a03ae-80be-4d7f-9e90-425ecd8a22d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.10.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=oaN2xRQaWVTOYpGBZr5aFVsI087Wpj&prompt=consent&token_usage=remote&access_type=offline&code_challenge=4UKi2j0r2EE6R22ncspjClG6CE_fzo8agKuzGKgHIVc&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AQSTgQFmHrgo_OXPyvaflZO4dt43lMfh-yKThYytKaKIH_VpKdSJHzedrjHTlbrrMvBbmg\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Utility Functions (Text Extraction, Chunking, Embedding)"
      ],
      "metadata": {
        "id": "YhFNz8TJn2YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        pdf_path = Path(pdf_path)\n",
        "        if not pdf_path.is_file() or not pdf_path.suffix.lower() == '.pdf':\n",
        "            raise ValueError(\"The provided file is not a valid PDF.\")\n",
        "\n",
        "        text = \"\"\n",
        "        with fitz.open(pdf_path) as pdf_document:\n",
        "            for page_num in range(len(pdf_document)):\n",
        "                text += pdf_document[page_num].get_text()\n",
        "        return text\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"The specified PDF file was not found.\")\n",
        "        return None\n",
        "    except fitz.FileDataError:\n",
        "        print(\"The PDF file is corrupted or unreadable.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Text Chunking ---\n",
        "\n",
        "try:\n",
        "    SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
        "    TOKENIZER = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load models: {e}\")\n",
        "\n",
        "def is_meaningful(sentence, threshold=5):\n",
        "    sentence = sentence.strip()\n",
        "    if len(sentence) < threshold:\n",
        "        return False\n",
        "    if re.fullmatch(r\"[\\W\\d_]+\", sentence):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def validate_text_input(text, max_length=1_000_000):\n",
        "    if not isinstance(text, str):\n",
        "        raise ValueError(\"Input text must be a string.\")\n",
        "    if len(text) > max_length:\n",
        "        raise ValueError(\"Input text is too large to process.\")\n",
        "    return text.strip()\n",
        "\n",
        "def smart_chunk_spacy_by_paragraph(text):\n",
        "    text = validate_text_input(text)\n",
        "    paragraphs = [para.strip() for para in text.split(\"\\n\") if is_meaningful(para)]\n",
        "    return paragraphs\n",
        "\n",
        "def smart_chunk_spacy(text):\n",
        "    text = validate_text_input(text)\n",
        "    doc = SPACY_NLP(text)\n",
        "    sentences = [sent.text for sent in doc.sents if is_meaningful(sent.text)]\n",
        "    return sentences\n",
        "\n",
        "def smart_chunk_spacy_advanced(text, min_chunk_length=50, max_chunk_length=500):\n",
        "    text = validate_text_input(text)\n",
        "    raw_paragraphs = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).split(\"\\n\\n\")\n",
        "    refined_paragraphs = []\n",
        "    for paragraph in raw_paragraphs:\n",
        "        if len(paragraph.strip()) < min_chunk_length:\n",
        "            continue\n",
        "        doc = SPACY_NLP(paragraph)\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "        for sent in doc.sents:\n",
        "            sent_text = sent.text.strip()\n",
        "            if current_length + len(sent_text) > max_chunk_length:\n",
        "                refined_paragraphs.append(\" \".join(current_chunk).strip())\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "            current_chunk.append(sent_text)\n",
        "            current_length += len(sent_text)\n",
        "        if current_chunk:\n",
        "            refined_paragraphs.append(\" \".join(current_chunk).strip())\n",
        "    return refined_paragraphs\n",
        "\n",
        "def smart_chunk_transformers(text, max_tokens=128):\n",
        "    text = validate_text_input(text)\n",
        "    tokens = TOKENIZER(text, truncation=False, return_tensors=\"pt\")\n",
        "    chunks = [text[i:i+max_tokens] for i in range(0, len(tokens['input_ids'][0]), max_tokens)]\n",
        "    return chunks\n",
        "\n",
        "# --- Vector Database (FAISS) Utilities ---\n",
        "from pathlib import Path\n",
        "# Load the embedding model globally\n",
        "try:\n",
        "    MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load the embedding model: {e}\")\n",
        "\n",
        "def validate_text_chunks(text):\n",
        "    if isinstance(text, str):\n",
        "        text = [text]\n",
        "    if not isinstance(text, list) or not all(isinstance(t, str) for t in text):\n",
        "        raise ValueError(\"Input must be a string or a list of strings.\")\n",
        "    return [t.strip() for t in text if t.strip()]\n",
        "\n",
        "def generate_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    chunks = validate_text_chunks(chunks)\n",
        "    embeddings = MODEL.encode(chunks, convert_to_tensor=False, show_progress_bar=True) # Add progress bar\n",
        "    embeddings = np.array(embeddings)\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    normalized_embeddings = embeddings / (norms + 1e-10)  # Add small value for numerical stability\n",
        "    return normalized_embeddings\n",
        "\n",
        "def store_in_faiss(embeddings, db_file=\"vector_db_cosine.index\"):\n",
        "    try:\n",
        "        dimension = embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)  # Use inner product (for cosine similarity with normalized vectors)\n",
        "        index.add(embeddings)\n",
        "        faiss.write_index(index, str(db_file)) #cast to string for cross-platform\n",
        "        print(f\"FAISS index saved to {db_file}\")\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to store FAISS index: {e}\")\n",
        "\n",
        "def load_faiss_index(db_file):\n",
        "    try:\n",
        "        db_file = Path(db_file).resolve()\n",
        "        return faiss.read_index(str(db_file))\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load FAISS index: {e}\")\n",
        "\n",
        "def query_faiss_index(query_text, vector_index, chunks, model_name=\"all-MiniLM-L6-v2\", top_k=2):\n",
        "    try:\n",
        "        query_text = validate_text_chunks(query_text)\n",
        "        if not query_text:\n",
        "            raise ValueError(\"Query text cannot be empty.\")\n",
        "        query_embedding = MODEL.encode(query_text, convert_to_tensor=False)\n",
        "        query_embedding = np.array(query_embedding)\n",
        "        query_embedding = query_embedding / (np.linalg.norm(query_embedding, axis=1, keepdims=True) + 1e-10)\n",
        "        distances, indices = vector_index.search(query_embedding, top_k)\n",
        "        results = [(chunks[idx], distances[0][i], idx) for i, idx in enumerate(indices[0])]\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to query FAISS index: {e}\")\n",
        "\n",
        "print(\"Utility functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIFj92C-nwfm",
        "outputId": "19aa5606-99c5-49b5-c79a-32adfbd96e7d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utility functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Gemini Query Function"
      ],
      "metadata": {
        "id": "FueuQkhin8R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "def query_flash(question = \"What is APR?\", context_chunks = [], model_name=\"gemini-pro\", top_k=3, test = False):\n",
        "    \"\"\"\n",
        "    Queries a Large Language Model (LLM) with a question and relevant context chunks.\n",
        "\n",
        "    Args:\n",
        "        question (str): The question to be answered.\n",
        "        context_chunks (list): A list of tuples, each containing (text_chunk, similarity_score, index).\n",
        "        model_name (str): The name of the Gemini model to use. Defaults to \"gemini-pro\".\n",
        "        top_k (int): The number of top context chunks to use. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated answer and the relevant context chunks.\n",
        "              Returns an error message as a string in case of exceptions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not isinstance(question, str) or not question.strip():\n",
        "            raise ValueError(\"The question must be a non-empty string.\")\n",
        "        if not isinstance(context_chunks, list) or not all(\n",
        "            isinstance(chunk, (list, tuple)) for chunk in context_chunks\n",
        "        ):\n",
        "            raise ValueError(\"Context chunks must be a list of tuples or lists.\")\n",
        "\n",
        "        client = genai.Client(\n",
        "            vertexai=True,\n",
        "            project=userdata.get('project_id'),\n",
        "            location=userdata.get('location'),\n",
        "\n",
        "        )\n",
        "\n",
        "        # Construct the context\n",
        "        context = \" \".join([context_chunk[0] for context_chunk in context_chunks])\n",
        "\n",
        "        # Prompt Engineering\n",
        "        prompt = (\n",
        "            f\"You are a legal assistant specializing in contracts. \"\n",
        "            f\"Answer the question based on the following context, and cite the sources explicitly. \"\n",
        "            f\"Do not include any information not present in the provided context.\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\n\"\n",
        "            f\"Question: {question}\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "        model = \"gemini-2.0-flash-lite-001\"\n",
        "        contents = [prompt\n",
        "        ]\n",
        "        generate_content_config = types.GenerateContentConfig(\n",
        "            temperature = 1,\n",
        "            top_p = 0.95,\n",
        "            max_output_tokens = 8192,\n",
        "            response_modalities = [\"TEXT\"],\n",
        "            safety_settings = [types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "            threshold=\"OFF\"\n",
        "            ),types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "            threshold=\"OFF\"\n",
        "            ),types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "            threshold=\"OFF\"\n",
        "            ),types.SafetySetting(\n",
        "            category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "            threshold=\"OFF\"\n",
        "            )],\n",
        "        )\n",
        "\n",
        "        if test:\n",
        "            res = client.models.generate_content_stream(model = model,\n",
        "                    contents = [\"Write a short story about a kitten\"],\n",
        "                    config = generate_content_config,)\n",
        "            for chunk in res:\n",
        "                print(chunk.text)\n",
        "            return\n",
        "\n",
        "        # Generate content (streaming)\n",
        "        response_stream = client.models.generate_content_stream(model = model,\n",
        "                    contents = contents,\n",
        "                    config = generate_content_config,)\n",
        "\n",
        "        # Collect the streamed response\n",
        "        generated_answer = \"\"\n",
        "        for chunk in response_stream:\n",
        "            generated_answer += chunk.text\n",
        "\n",
        "        relevant_chunks = context_chunks[:top_k]  # Top k chunks for citation\n",
        "        return {\"answer\": generated_answer.strip(), \"relevant_context\": relevant_chunks}\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "\n",
        "print(\"Gemini query function defined.\")\n",
        "query_flash(test = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60yNWmjpn4dx",
        "outputId": "6e9c0328-ed6c-47e9-884a-87588b98fe44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini query function defined.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pip\n",
            "kin was a wisp of smoke and mischief, a tiny whirlwind of grey fluff\n",
            ". He was barely bigger than a teacup, with enormous, emerald eyes that reflected\n",
            " the world in miniature. He lived in the attic of a dusty old house, his kingdom a land of forgotten trunks, cobweb castles, and sunlight-dappled dust\n",
            " motes.\n",
            "\n",
            "His days were a symphony of exploration. He would stalk the frayed edges of the carpet, transforming into a miniature panther, his tiny body low\n",
            " to the ground. He would bat at the dancing dust motes with delicate paws, imagining them to be mischievous fairies. He would climb the towering legs of the old chaise lounge, his claws clicking against the wood, scaling Everest in his tiny world\n",
            ".\n",
            "\n",
            "His greatest treasure was a sunbeam that streamed through a crack in the boarded-up window. In that golden rectangle, he would bask, purring like a tiny engine, his fur turning to a shimmering silver. Sometimes, he’d\n",
            " catch a stray moth, a furry snack that brought a surge of triumph and a playful flick of his tail.\n",
            "\n",
            "One day, a new scent infiltrated his attic haven – the sweet, yeasty aroma of baking. Curiosity, a powerful force within a tiny kitten, propelled him towards the source. He squeezed through a crack\n",
            " in the floorboards and tumbled, blinking, into the kitchen.\n",
            "\n",
            "The world exploded with new sensations. A giant, looming leg (a human's!) loomed over him. The air vibrated with the warmth of the oven. A large, pink face, filled with surprise, appeared.\n",
            "\n",
            "\"Oh my\n",
            " goodness!\" a voice exclaimed. \"What are *you* doing here?\"\n",
            "\n",
            "Pipkin, bewildered but not scared, simply stared up at the giant, his emerald eyes gleaming. He knew, instinctively, that he was safe.\n",
            "\n",
            "The giant picked him up, her hands surprisingly gentle. He felt the soft fabric of her apron\n",
            " against his fur. She smelled of flour and vanilla.\n",
            "\n",
            "\"You're a little explorer, aren't you?\" she cooed.\n",
            "\n",
            "Pipkin nuzzled his head against her chin, a tiny purr rumbling in his chest. He had found a new sunbeam, a new world to explore, a new\n",
            " kind of magic – the magic of kindness. He had found a home. And as he looked up at the giant's smiling face, he knew his adventures had just begun. The attic, his kingdom, would still be there, but now he had a queen, and a whole world outside the crack in the floorboards\n",
            " to explore. The world, as it turned out, was a very exciting place indeed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Question Answering Function"
      ],
      "metadata": {
        "id": "4Dqo7GOhoBcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(pdf_path, query_text, relevance_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Main function to perform question answering on a PDF document.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.  (Will be a temporary path in Colab)\n",
        "        query_text (str): The question to ask.\n",
        "        relevance_threshold (float): Minimum similarity score for a chunk.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nExtracting text from the PDF...\")\n",
        "    try:\n",
        "        extracted_text = extract_text_from_pdf(pdf_path)\n",
        "        print(\"Text extracted successfully.\")\n",
        "        if not extracted_text:\n",
        "            print(\"Error: Failed to extract text. Document might be empty/unreadable.\")\n",
        "            return  # Exit if extraction fails\n",
        "\n",
        "        print(\"\\nChunking the extracted text...\")\n",
        "        chunks = smart_chunk_spacy_advanced(extracted_text)  # Use advanced chunker\n",
        "        if not chunks:\n",
        "            print(\"Error: Failed to create text chunks.\")\n",
        "            return\n",
        "        print(f\"Text chunked into {len(chunks)} chunks.\")\n",
        "\n",
        "        print(\"\\nGenerating embeddings for the chunks...\")\n",
        "        embeddings = generate_embeddings(chunks)\n",
        "        print(\"Embeddings generated.\")\n",
        "\n",
        "        print(\"\\nStoring embeddings in FAISS index...\")\n",
        "        index = store_in_faiss(embeddings)  # Use default filename\n",
        "        print(\"FAISS index created and stored.\")\n",
        "\n",
        "        # No need to reload the index immediately, we just created it!\n",
        "\n",
        "        print(f\"\\nQuerying FAISS index with: '{query_text}'...\")\n",
        "        results = query_faiss_index(query_text, index, chunks, top_k=5)\n",
        "\n",
        "        # Filter results by relevance threshold\n",
        "        context_chunks = [(text, dis, idx) for text, dis, idx in results if dis > relevance_threshold]\n",
        "\n",
        "        print(\"\\nResults from FAISS index:\")\n",
        "        for idx, (text, score, doc_id) in enumerate(results):\n",
        "          if score > relevance_threshold:\n",
        "            print(f\"{idx}. Score: {score:.4f}, Document ID: {doc_id}\\n{text[:200]}...\\n\")\n",
        "        print(f\"Found {len(context_chunks)} relevant context chunks.\")\n",
        "\n",
        "\n",
        "        print(\"\\nQuerying the Gemini model for an answer...\")\n",
        "        answer = query_flash(query_text, context_chunks)  # Use Gemini query\n",
        "        if isinstance(answer, str) and \"Error\" in answer: #check for errors from query_flash\n",
        "          print(f\"Error querying Gemini: {answer}\")\n",
        "          return\n",
        "\n",
        "        print(\"\\nGenerated Answer:\")\n",
        "        print(answer[\"answer\"])\n",
        "        print(\"\\nCited Context:\")\n",
        "        for text, _, idx in answer[\"relevant_context\"]:\n",
        "            print(f\"\\tSource {idx}: {text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred in main(): {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Main function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi4_cthJn_RP",
        "outputId": "067ea01f-7169-4009-c483-b25e2ce59542"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get API Key, Project ID, and Location, Upload PDF, and Run!"
      ],
      "metadata": {
        "id": "846edhDIoGTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# File upload\n",
        "print(\"Upload your PDF file:\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    print(\"No file uploaded.  Exiting.\")\n",
        "    sys.exit(1)  # Exit if no file\n",
        "\n",
        "pdf_filename = list(uploaded.keys())[0]  # Get the filename\n",
        "print(f\"Uploaded file: {pdf_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "s7Y8WCaFoG_B",
        "outputId": "950d8a4d-e6a4-4ff7-eec1-f62a461c3b2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-31340f80-95f1-45b9-8dc4-8ec2d73d3051\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-31340f80-95f1-45b9-8dc4-8ec2d73d3051\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ExampleCo - NDA - John Appleseed.pdf to ExampleCo - NDA - John Appleseed (4).pdf\n",
            "Uploaded file: ExampleCo - NDA - John Appleseed (4).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the question\n",
        "query = \"what state is this contract binding at \"  #@param {type:\"string\"}\n",
        "\n",
        "# Run the main function\n",
        "main(pdf_filename, query)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a0a0363069334ac98aa1c577358e5352",
            "888b1b06039944eaa5b6960033feca9e",
            "93578cfbe6b144d0bcdedce648242313",
            "a290d7ae94b446318d4f7170a1fe2206",
            "2fc78fc04c284f8fa944158b4da9b6ed",
            "87055f9bffc54e03b82a7456dc430c44",
            "3865e6a4d341433a9347e3ffdc8702b2",
            "790364264b81459facef7fcb28a967ac",
            "b8561c2e80a14ca3aa6746d092a16911",
            "745e564d11014fe8b6df76a5489bb384",
            "a589be05ea53482bb5df103fb115e50f"
          ]
        },
        "id": "a9VcNvhTyP_F",
        "outputId": "fbc4b07a-fbbe-465a-87ef-7afc59676c85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting text from the PDF...\n",
            "Text extracted successfully.\n",
            "\n",
            "Chunking the extracted text...\n",
            "Text chunked into 25 chunks.\n",
            "\n",
            "Generating embeddings for the chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0a0363069334ac98aa1c577358e5352"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generated.\n",
            "\n",
            "Storing embeddings in FAISS index...\n",
            "FAISS index saved to vector_db_cosine.index\n",
            "FAISS index created and stored.\n",
            "\n",
            "Querying FAISS index with: 'what state is this contract binding at '...\n",
            "\n",
            "Results from FAISS index:\n",
            "0. Score: 0.3535, Document ID: 18\n",
            "The Recipient shall hold harmless and indemnify the Company, as well as the\n",
            "shareholders, officers, directors, employees, agents and representatives of the Company, from\n",
            "3\n",
            "- -\n",
            "4853-1848-6325.v1\n",
            "and ag...\n",
            "\n",
            "1. Score: 0.3384, Document ID: 22\n",
            "The covenants and agreements set forth in this Agreement are each deemed\n",
            "separate and independent, and if any such covenant or agreement is determined by any court of\n",
            "competent jurisdiction to be inva...\n",
            "\n",
            "2. Score: 0.3354, Document ID: 15\n",
            "The Company makes no warranty whatsoever relating to the\n",
            "Confidential Information and the use to be made thereof by the Recipient, and the Company\n",
            "disclaims all implied warranties. 12....\n",
            "\n",
            "3. Score: 0.3174, Document ID: 0\n",
            "...\n",
            "\n",
            "4. Score: 0.3083, Document ID: 13\n",
            "The obligations of the Recipient under this Agreement shall continue for a period\n",
            "of five (5) years from the later of (i) the date of this Agreement and (ii) the date of the last\n",
            "disclosure of Confide...\n",
            "\n",
            "Found 5 relevant context chunks.\n",
            "\n",
            "Querying the Gemini model for an answer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Answer:\n",
            "I am sorry, but the provided text does not specify what state this contract is binding at.\n",
            "\n",
            "Cited Context:\n",
            "\tSource 18: The Recipient shall hold harmless and indemnify the Company, as well as the\n",
            "shareholders, officers, directors, employees, agents and representatives of the Company, from\n",
            "3\n",
            "- -\n",
            "4853-1848-6325.v1\n",
            "and against any and all claims, judgments, obligations, costs, awards, expenses (including,\n",
            "without limitation, reasonable attorneys’ fees and costs) and liabilities of every kind arising from\n",
            "any use made by the Recipient of the Confidential Information.\n",
            "15.\n",
            "\tSource 22: The covenants and agreements set forth in this Agreement are each deemed\n",
            "separate and independent, and if any such covenant or agreement is determined by any court of\n",
            "competent jurisdiction to be invalid or unenforceable for any reason, including, without\n",
            "limitation, by reason of such covenant or agreement extending for too great a period of time or\n",
            "over too great a geographical area, or by reason of its being too extensive in any other respect,\n",
            "such covenant or agreement, to the specific extent that it is unenforceable, shall be deemed\n",
            "automatically deleted from this Agreement and shall be interpreted to extend only over the\n",
            "maximum period of time and geographical area, and to the maximum extent in all other respects,\n",
            "as to which it is valid and enforceable, in order to effectuate the parties’ intent to the greatest\n",
            "extent possible.\n",
            "\tSource 15: The Company makes no warranty whatsoever relating to the\n",
            "Confidential Information and the use to be made thereof by the Recipient, and the Company\n",
            "disclaims all implied warranties. 12.\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srTH6lGoX9yF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}